{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import os.path\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import imodelsx.viz\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import data\n",
    "import joblib\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import spearmanr\n",
    "LEVELS = ['Very Negative', 'Negative', 'Neutral',\n",
    "          'No response', 'Positive', 'Very Positive']\n",
    "\n",
    "# get data for example site\n",
    "files_dict = data.load_files_dict_single_site()\n",
    "site = 'Charlotte'\n",
    "df = files_dict[site]\n",
    "qs, responses_df, themes_df = data.split_single_site_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze human results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load human annotations into a df called `annotations_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = {\n",
    "    'hum1': 'human1',\n",
    "    'hum2': 'human2',\n",
    "    'hum3': 'human3',\n",
    "}\n",
    "\n",
    "# template = pd.read_pickle('../figs/human/sentiment_template.pkl')\n",
    "annotations_df = pd.read_csv('../figs/human/sentiment_template.csv')\n",
    "for k, v in annots.items():\n",
    "    hum = pd.read_csv(f'../figs/human/collected/sentiment_{v}.csv', skiprows=1)\n",
    "\n",
    "    def remove_all_whitespace(s):\n",
    "        return ''.join(s.split())\n",
    "\n",
    "    # check for matching index, value range\n",
    "    assert hum.shape[0] == annotations_df.shape[\n",
    "        0], f'Shape mismatch for {k}: {hum.shape[0]} vs {annotations_df.shape[0]}'\n",
    "    assert np.all(hum['Response number'].astype(str).apply(remove_all_whitespace).values ==\n",
    "                  annotations_df['Response number'].astype(str).apply(remove_all_whitespace).values), f'Error for hum {k}'\n",
    "\n",
    "    # add col\n",
    "    annotations_df[k] = hum['Rating'].values.astype(int)\n",
    "    assert np.all(annotations_df[k].values >= 1)\n",
    "    assert np.all(annotations_df[k].values <= 5)\n",
    "\n",
    "checkpoints_all = [\n",
    "    'gpt-4',\n",
    "    'gpt-35-turbo',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    'mistralai/Mistral-7B-v0.1',  # 'mistralai/Mixtral-8x7B-v0.1'\n",
    "]\n",
    "\n",
    "\n",
    "def find_starting_number(s):\n",
    "    # if s starts with a number or a number with decimal places, return that number\n",
    "    # otherwise return nan\n",
    "    s = str(s).strip()\n",
    "    if s == 'nan':\n",
    "        return np.nan\n",
    "    ans = ''\n",
    "    while len(s) > 0 and (s[0].isdigit() or s[0] == '.'):\n",
    "        ans += s[0]\n",
    "        s = s[1:]\n",
    "    return float(ans) if len(ans) > 0 else np.nan\n",
    "\n",
    "\n",
    "for checkpoint in checkpoints_all:\n",
    "    sites = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "    sent_dfs = []\n",
    "    for site in sites:\n",
    "        sent_df = joblib.load(join(\n",
    "            data.PROCESSED_DIR, f'sentiments_df_{site}_{checkpoint.split(\"/\")[-1]}.pkl'))\n",
    "        sent_dfs.append(sent_df)\n",
    "\n",
    "    sent_dfs[0].columns = np.arange(0, 11)\n",
    "    sent_dfs[1].columns = np.arange(11, 22)\n",
    "    sent_dfs[2].columns = np.arange(22, 33)\n",
    "    sent_llm_full = pd.concat(sent_dfs, axis=1).values\n",
    "    annotations_df[checkpoint] = annotations_df.apply(\n",
    "        lambda row: sent_llm_full[row['Question number'], row['Response number']], axis=1)\n",
    "    annotations_df[checkpoint] = annotations_df[checkpoint].apply(\n",
    "        find_starting_number)\n",
    "llms_to_ensemble = ['gpt-4', 'gpt-35-turbo',\n",
    "                    'mistralai/Mistral-7B-v0.1',  # 'meta-llama/Llama-2-70b-hf',\n",
    "                    # 'mistralai/Mixtral-8x7B-v0.1',\n",
    "                    ]\n",
    "\n",
    "annotations_df['Human ensemble'] = annotations_df[[\n",
    "    'hum1', 'hum2', 'hum3']].mean(axis=1)\n",
    "annotations_df = annotations_df.rename(columns={\n",
    "    'hum1': 'Human 1',\n",
    "    'hum2': 'Human 2',\n",
    "    'hum3': 'Human 3',\n",
    "})\n",
    "\n",
    "\n",
    "annotations_df.columns = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), annotations_df.columns))\n",
    "checkpoints_all = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), checkpoints_all))\n",
    "llms_to_ensemble = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "annotations_df['LLM ensemble'] = annotations_df[llms_to_ensemble].mean(axis=1)\n",
    "\n",
    "\n",
    "# note: GPT-4 and GPT-3.5 Turbo got swapped while running, need to swap their column vals back\n",
    "col_gpt4 = annotations_df['GPT-4'].copy()\n",
    "col_gpt35 = annotations_df['GPT-3.5 Turbo'].copy()\n",
    "annotations_df['GPT-4'] = col_gpt35\n",
    "annotations_df['GPT-3.5 Turbo'] = col_gpt4\n",
    "\n",
    "# round ensemble\n",
    "annotations_df['LLM ensemble'] = annotations_df['LLM ensemble'].round()\n",
    "annotations_df['Human ensemble'] = annotations_df['Human ensemble'].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlations between the columns of `annotations_df` and storr in `corr_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to apply this across all llms\n",
    "notna = np.ones(len(annotations_df), dtype=bool)\n",
    "for ckpt in checkpoints_all:\n",
    "    notna &= annotations_df[ckpt].apply(find_starting_number).notna()\n",
    "print('num annots', notna.sum())\n",
    "\n",
    "checkpoints_all_hum = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble'] + \\\n",
    "    ['LLM ensemble'] + checkpoints_all\n",
    "corr = np.zeros((len(checkpoints_all_hum), len(checkpoints_all_hum)))\n",
    "for r, cr in enumerate(checkpoints_all_hum):\n",
    "    for c, cc in enumerate(checkpoints_all_hum):\n",
    "        x = annotations_df[cr][notna].astype(int)\n",
    "        y = annotations_df[cc][notna].astype(int)\n",
    "        # corr[r, c] = np.corrcoef(x, y)[0, 1]  # spearmanr(x, y)\n",
    "        corr[r, c] = cohen_kappa_score(x, y, weights='quadratic')\n",
    "\n",
    "# convert to df\n",
    "# labels[0] = 'Human 1'\n",
    "# labels[1] = 'Human 2'\n",
    "# labels[2] = 'Human 3'\n",
    "labels = checkpoints_all_hum\n",
    "print(labels, corr.shape)\n",
    "corr_df = pd.DataFrame(corr,\n",
    "                       index=labels,\n",
    "                       columns=labels)\n",
    "\n",
    "# sort by corr with human\n",
    "ind = corr_df.sort_values(by='Human ensemble', ascending=False).index\n",
    "ind.values[:4] = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble']\n",
    "corr_df = corr_df.reindex(ind)[ind]\n",
    "\n",
    "# Replace correlations with Human ensemble by excluding the human\n",
    "cols = ['Human 1', 'Human 2', 'Human 3']\n",
    "hum_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = annotations_df[[\n",
    "        col for col in cols if col != c]].mean(axis=1)\n",
    "    hum_corrs.append(np.corrcoef(\n",
    "        annotations_df[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(hum_corrs)\n",
    "corr_df.loc[cols, 'Human ensemble'] = hum_corrs\n",
    "corr_df.loc['Human ensemble', cols] = hum_corrs\n",
    "\n",
    "# Replace correlations with LLM ensemble by excluding the LLM\n",
    "cols = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "llm_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = annotations_df[[\n",
    "        col for col in cols if col != c]].mean(axis=1)\n",
    "    llm_corrs.append(np.corrcoef(\n",
    "        annotations_df[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(llm_corrs)\n",
    "# corr_df.loc[cols, 'LLM ensemble'] = llm_corrs\n",
    "# corr_df.loc['LLM ensemble', cols] = llm_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make main plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'LLM Ensemble' row and col\n",
    "corr_df = corr_df.drop('LLM ensemble', axis=0)\n",
    "corr_df = corr_df.drop('LLM ensemble', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = corr_df.iloc[1:, 1:]\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "mask[np.diag_indices_from(mask)] = False\n",
    "\n",
    "rename = {\n",
    "    'Human 1': 'Human reviewer 1',\n",
    "    'Human 2': 'Human reviewer 2',\n",
    "    'Human 3': 'Human reviewer 3',\n",
    "    'Human ensemble': 'Mean human reviewer',\n",
    "    # 'LLM ensemble': 'Mean LLM',\n",
    "}\n",
    "corr_df = corr_df.rename(columns=rename, index=rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    corr_df.loc['Mean human reviewer', f'Human reviewer {i}'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rename Mean human reviewer to Median human reviewer\n",
    "# corr_df = corr_df.rename(index={'Mean human reviewer': 'Median human reviewer'}, columns={\n",
    "#                          'Mean human reviewer': 'Median human reviewer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    corr_df,\n",
    "    annot=True, fmt='.2f',\n",
    "    cmap=sns.color_palette(\"Blues\", as_cmap=True), cbar_kws={'label': 'Cohen\\'s Kappa'},\n",
    "    mask=mask,\n",
    ")\n",
    "\n",
    "# outline the first row of the elements in the heatmap\n",
    "color = '#fa755a'\n",
    "lw = 3\n",
    "roffset = 3.5\n",
    "coffset = 0.5\n",
    "shape = corr_df.shape\n",
    "r = 0\n",
    "# color = 'gray'\n",
    "alpha = 1\n",
    "for c in range(3, shape[1]):\n",
    "    rx = r + roffset\n",
    "    cx = c + coffset\n",
    "    if c == 2:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx - 0.5, cx - 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    if c == shape[1] - 1:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx + 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx - 0.5, rx - 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx + 0.5, rx + 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "\n",
    "roffset = 0.5\n",
    "coffset = 0.5\n",
    "c = 3\n",
    "plt.plot([3 - 0.5 + 0.5, 3 + 0.5 + 0.5],\n",
    "         [3 + 0.5 - 0.5, 3 + 0.5 - 0.5], color=color, lw=lw, alpha=alpha)\n",
    "\n",
    "# set the color of first three xticklabels and yticklabels to blue\n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "for i, t in enumerate(ax.get_yticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "\n",
    "\n",
    "plt.xlim(-.2, shape[0])\n",
    "plt.ylim(shape[1] + 0.2, -.2)\n",
    "# plt.ylabel('Annotator')\n",
    "# plt.xlabel('Annotator')\n",
    "plt.savefig('../figs/sentiment_correlation.pdf', bbox_inches='tight')\n",
    "plt.savefig('../figs/sentiment_correlation.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6905433111225012, array([0.60857698, 0.75873471]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kappa_interval(x, y, alpha=0.05):\n",
    "    notna = x.notna() & y.notna()\n",
    "    x = x[notna].round().astype(int).values\n",
    "    y = y[notna].round().astype(int).values\n",
    "    kappa_true = cohen_kappa_score(x, y, weights='quadratic')\n",
    "    kappas = []\n",
    "    for i in range(1000):\n",
    "        idx = np.random.choice(len(x), len(x) * 3)\n",
    "        kappas.append(cohen_kappa_score(x[idx], y[idx], weights='quadratic'))\n",
    "\n",
    "    interval = np.percentile(kappas, [100 * alpha/2, 100 * (1 - alpha/2)])\n",
    "    return kappa_true, interval\n",
    "\n",
    "\n",
    "kappa_interval(annotations_df['Human ensemble'], annotations_df['GPT-4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human-Human\n",
    "pairings = [\n",
    "    ('Human 1', 'Human 2'),\n",
    "    ('Human 2', 'Human 3'),\n",
    "    ('Human 1', 'Human 3'),\n",
    "]\n",
    "kappas = []\n",
    "for p in pairings:\n",
    "    kappa = kappa_interval(annotations_df[p[0]], annotations_df[p[1]])[0]\n",
    "    kappas.append(kappa)\n",
    "np.mean(kappas), np.std(kappas) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8395513143550085, 0.034147483194124015)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human-True\n",
    "pairings = [\n",
    "    ('Human 1', 'Human 2'),\n",
    "    ('Human 2', 'Human 3'),\n",
    "    ('Human 1', 'Human 3'),\n",
    "]\n",
    "kappas = []\n",
    "for p in pairings:\n",
    "    kappa = kappa_interval(annotations_df[p[0]], annotations_df[p[1]])[0]\n",
    "    kappas.append(kappa)\n",
    "np.mean(kappas), np.std(kappas) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_confidence_interval(r, alpha, n):\n",
    "    def _r_to_z(r):\n",
    "        return math.log((1 + r) / (1 - r)) / 2.0\n",
    "\n",
    "    def _z_to_r(z):\n",
    "        e = math.exp(2 * z)\n",
    "        return ((e - 1) / (e + 1))\n",
    "\n",
    "    z = _r_to_z(r)\n",
    "    se = 1.0 / math.sqrt(n - 3)\n",
    "    z_crit = scipy.stats.norm.ppf(1 - alpha/2)  # 2-tailed z critical value\n",
    "\n",
    "    lo = z - z_crit * se\n",
    "    hi = z + z_crit * se\n",
    "\n",
    "    # Return a sequence\n",
    "    return (round(_z_to_r(lo), 2), round(_z_to_r(hi), 2))\n",
    "\n",
    "\n",
    "print('interval human-LLM', r_confidence_interval(0.741484, 0.05, 123))\n",
    "# print('interval annotators', np.mean([0.813302, 0.850014, 0.935228]), r_confidence_interval(\n",
    "#     np.mean([0.813302, 0.850014, 0.935228]), 0.05, 123))\n",
    "# print('interval each human-LLM',\n",
    "#       [r_confidence_interval(x, 0.05, 123) for x in [0.845929, 0.915878, 0.941113]])\n",
    "# print('interval human1-human2',\n",
    "#       [r_confidence_interval(0.813302, 0.05, 123)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = corr_df.loc['Mean human reviewer', [\n",
    "    'Human reviewer 1', 'Human reviewer 2', 'Human reviewer 3']]\n",
    "print('Human ensemble vs human', vals.mean(), vals.sem(ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = annotations_df['LLM ensemble'][notna]\n",
    "x2 = annotations_df['Human ensemble'][notna]\n",
    "\n",
    "# calculate pearson correlation and error of the corr\n",
    "corr = np.corrcoef(x1, x2)[0, 1]\n",
    "err = np.sqrt((1 - corr**2) / (len(x1) - 2))\n",
    "print('LLM ensemble vs human ensemble corr', corr, 'err', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with some jitter\n",
    "def jitter(values, j, min=1, max=5):\n",
    "    values = values + np.random.uniform(-j, j, values.shape)\n",
    "    return np.clip(values, min, max)\n",
    "\n",
    "\n",
    "x = annotations_df['Human ensemble']\n",
    "y = annotations_df['LLM ensemble']\n",
    "plt.plot(jitter(x, 0.15), jitter(y, 0.15), 'o', alpha=0.5)\n",
    "plt.xlabel('Annotator ensemble sentiment score')\n",
    "plt.ylabel('LLM ensemble sentiment score')\n",
    "\n",
    "plt.plot([1, 5], [1, 5], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of time LLM is more extreme than human',\n",
    "      (np.abs(y[notna] - 3) > np.abs(x[notna] - 3)).mean())\n",
    "print('Fraction of time LLM reverses polarity',\n",
    "      (((x >= 4) & (y <= 2)) | ((x <= 2) & (y >= 4)))[notna].mean()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
