{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import os.path\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import imodelsx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import imodelsx.viz\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import data\n",
    "import joblib\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.stats import spearmanr\n",
    "LEVELS = ['Very Negative', 'Negative', 'Neutral',\n",
    "          'No response', 'Positive', 'Very Positive']\n",
    "\n",
    "# get data for example site\n",
    "files_dict = data.load_files_dict_single_site()\n",
    "site = 'Charlotte'\n",
    "df = files_dict[site]\n",
    "qs, responses_df, themes_df = data.split_single_site_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate sentiment plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dfs = []\n",
    "#\n",
    "sites = ['Atlanta', 'Columbus', 'WashingtonDC'] + \\\n",
    "    ['Charlotte', 'Houston', 'Portland'] + \\\n",
    "    ['Dallas', 'Seattle', 'Tucson']\n",
    "for site in sites:\n",
    "    sent_df = joblib.load(\n",
    "        join(data.PROCESSED_DIR, f'sent_df_{site}_gpt-4.pkl'))\n",
    "    # sent_df = sent_df.sort_values(by=levels, ascending=False)\n",
    "    sent_dfs.append(sent_df)\n",
    "\n",
    "sum_df = pd.concat(sent_dfs).groupby(level=0).sum()\n",
    "\n",
    "# make plot\n",
    "# sum_df = sum_df.sort_values(by=LEVELS, ascending=False)\n",
    "# sum_index_sorted = sum_df.index\n",
    "mean = -2 * sum_df['Very Negative'] + -1 * sum_df['Negative'] + \\\n",
    "    1 * sum_df['Positive'] + 2 * sum_df['Very Positive']\n",
    "sum_index_sorted = mean.sort_values().index\n",
    "sum_df = sum_df.reindex(sum_index_sorted)\n",
    "\n",
    "colors = sns.diverging_palette(20, 220, n=6).as_hex()\n",
    "colors = colors[:2] + ['#ddd', '#eee'] + colors[-2:]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "gs = GridSpec(1, 4, width_ratios=[2, 1, 1, 1], wspace=0.025)\n",
    "\n",
    "\n",
    "# first plot\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "sum_df.plot(kind='barh', stacked=True, color=colors, ax=ax1, legend=False)\n",
    "plt.xlim(0, sum_df.sum(axis=1).max())\n",
    "# remove label for final xtick\n",
    "labels = [str(i) if i % 10 == 0 else '' for i in range(\n",
    "    0, int(sum_df.sum(axis=1).max()) + 1, 10)]\n",
    "labels[-1] = ''\n",
    "plt.xticks(range(0, int(sum_df.sum(axis=1).max()) + 1, 10),\n",
    "           labels=labels)\n",
    "plt.yticks(range(46),\n",
    "           labels=[f'{len(sent_df) - i}. {v}' for i, v in enumerate(\n",
    "               df['Domain'].values[sum_df.index.values])])\n",
    "fig.legend(bbox_to_anchor=(0.5, 0.95), loc='center',\n",
    "           ncol=3, title='Sentiment polarity')\n",
    "plt.xlabel('Answer count')\n",
    "plt.title('All sites', fontweight='bold')\n",
    "plt.ylabel('Question number and domain')\n",
    "\n",
    "sites_examples = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "for i, site in enumerate(sites_examples):\n",
    "    ax = fig.add_subplot(gs[i+1])\n",
    "    sent_df = sent_dfs[i]\n",
    "    sent_df = sent_df.reindex(sum_index_sorted)\n",
    "    sent_df.plot(kind='barh', stacked=True, color=colors, ax=ax, legend=False)\n",
    "    plt.xlim(0, sent_df.sum(axis=1).max())\n",
    "    plt.yticks([])\n",
    "    plt.ylabel('')\n",
    "    # plt.title(data.RENAME_SITE_DICT.get(site, site))\n",
    "    plt.title(f'Site {\"ABC\"[i]}')\n",
    "plt.savefig(f'../figs/sentiment_agg.pdf', bbox_inches='tight')\n",
    "plt.savefig(f'../figs/sentiment_agg.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_responded = sum_df.sum().sum() - sum_df['No response'].sum().sum()\n",
    "print(total_responded, 'responses', sum_df['No response'].sum(), 'no response')\n",
    "print('breakdown', sum_df.sum(), round((100 * sum_df.sum() / total_responded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no limit display\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    d = df.iloc[sum_index_sorted][::-1][['Domain', 'Subcategory']]\n",
    "    d['Question order'] = d.index\n",
    "    d.insert(loc=0, column='Question number', value=np.arange(1, len(d) + 1))\n",
    "    # display(d)\n",
    "    d.to_csv('../figs/question_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original function filter 10 questions that have many themes (these tend to be more interesting)\n",
    "# # select questions\n",
    "# def get_num_themes(df):\n",
    "#     num_themes_list = []\n",
    "#     for question_num in range(len(df)):\n",
    "#         question, responses, theme_dict = data.get_data_for_question_single_site(\n",
    "#             question_num=question_num, qs=qs, responses_df=responses_df, themes_df=themes_df)\n",
    "#         num_themes_list.append(len(theme_dict))\n",
    "#     return np.array(num_themes_list)\n",
    "#     # df['num_themes'] = num_themes_list\n",
    "#     # return df\n",
    "\n",
    "\n",
    "# num_themes = np.zeros(46)\n",
    "# SITES = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "# for site in sites:\n",
    "#     df = files_dict[site]\n",
    "#     num_themes += get_num_themes(df)\n",
    "# idx = pd.Series(num_themes).sort_values(ascending=False)\n",
    "# questions_selected = idx.index[:10]\n",
    "\n",
    "# instead pick 3 most positive, 3 middle, and 3 most negative\n",
    "mid = 46 // 2\n",
    "sorted_qs = list(sum_index_sorted)\n",
    "questions_selected = sorted_qs[:3] + sorted_qs[mid-1:mid+2] + sorted_qs[-3:]\n",
    "\n",
    "# save these questions\n",
    "pd.Series(questions_selected).to_csv(\n",
    "    '../figs/human/sentiment_questions_selected.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select random answers to questions (up to 15 per question)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in responses from all sites\n",
    "# resps_dict = defaultdict(list)\n",
    "# for question_num in questions_selected:\n",
    "#     for site in ['Atlanta', 'Columbus', 'WashingtonDC']:\n",
    "#         df = files_dict[site]\n",
    "#         question, responses, theme_dict = data.get_data_for_question_single_site(\n",
    "#             question_num=question_num, qs=qs, responses_df=responses_df, themes_df=themes_df)\n",
    "#         resps_dict[question_num] += list(responses)\n",
    "# assert np.all(np.array(list(len(v) for v in resps_dict.values())) == 33)\n",
    "\n",
    "\n",
    "# # randomly select up to 15 non-nan responses for each question and record their indices (less if there are fewer than 15)\n",
    "# rng = np.random.default_rng(13)\n",
    "# resps_idx_selected = defaultdict(list)\n",
    "# resps_selected = {}\n",
    "# for question_num in questions_selected:\n",
    "#     resps = resps_dict[question_num]\n",
    "#     indices = np.arange(33)[~pd.isna(resps)]\n",
    "#     indices_selected = rng.choice(\n",
    "#         indices, size=min(len(indices), 15), replace=False).tolist()\n",
    "#     resps_idx_selected[question_num] = indices_selected\n",
    "#     resps_selected[question_num] = [resps[i] for i in indices_selected]\n",
    "\n",
    "# # put into a big defaultdict\n",
    "# dd = defaultdict(list)\n",
    "# for question_num in questions_selected:\n",
    "#     for i, resp in enumerate(resps_selected[question_num]):\n",
    "#         dd['Question number'].append(question_num)\n",
    "#         dd['Response number'].append(resps_idx_selected[question_num][i])\n",
    "\n",
    "#         dd['Question'].append(qs[question_num])\n",
    "#         dd['Response'].append(resp)\n",
    "\n",
    "# # dump\n",
    "# with open('../figs/human/sentiment_idx_selected.json', 'w') as f:\n",
    "#     json.dump(resps_idx_selected, f, indent=4)\n",
    "# ddf = pd.DataFrame.from_dict(dd)\n",
    "# ddf.to_csv('../figs/human/sentiment_template.csv', index=False)\n",
    "# ddf.to_pickle('../figs/human/sentiment_template.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze human results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load human annotations into a df called `annotations_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = {\n",
    "    'hum1': 'human1',\n",
    "    'hum2': 'human2',\n",
    "    'hum3': 'human3',\n",
    "}\n",
    "\n",
    "# template = pd.read_pickle('../figs/human/sentiment_template.pkl')\n",
    "annotations_df = pd.read_csv('../figs/human/sentiment_template.csv')\n",
    "for k, v in annots.items():\n",
    "    hum = pd.read_csv(f'../figs/human/collected/sentiment_{v}.csv', skiprows=1)\n",
    "\n",
    "    def remove_all_whitespace(s):\n",
    "        return ''.join(s.split())\n",
    "\n",
    "    # check for matching index, value range\n",
    "    assert hum.shape[0] == annotations_df.shape[\n",
    "        0], f'Shape mismatch for {k}: {hum.shape[0]} vs {annotations_df.shape[0]}'\n",
    "    assert np.all(hum['Response number'].astype(str).apply(remove_all_whitespace).values ==\n",
    "                  annotations_df['Response number'].astype(str).apply(remove_all_whitespace).values), f'Error for hum {k}'\n",
    "\n",
    "    # add col\n",
    "    annotations_df[k] = hum['Rating'].values.astype(int)\n",
    "    assert np.all(annotations_df[k].values >= 1)\n",
    "    assert np.all(annotations_df[k].values <= 5)\n",
    "\n",
    "checkpoints_all = [\n",
    "    'gpt-4',\n",
    "    'gpt-35-turbo',\n",
    "    'meta-llama/Llama-2-70b-hf',\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    'mistralai/Mistral-7B-v0.1',  # 'mistralai/Mixtral-8x7B-v0.1'\n",
    "]\n",
    "\n",
    "\n",
    "def find_starting_number(s):\n",
    "    # if s starts with a number or a number with decimal places, return that number\n",
    "    # otherwise return nan\n",
    "    s = str(s).strip()\n",
    "    if s == 'nan':\n",
    "        return np.nan\n",
    "    ans = ''\n",
    "    while len(s) > 0 and (s[0].isdigit() or s[0] == '.'):\n",
    "        ans += s[0]\n",
    "        s = s[1:]\n",
    "    return float(ans) if len(ans) > 0 else np.nan\n",
    "\n",
    "\n",
    "for checkpoint in checkpoints_all:\n",
    "    sites = ['Atlanta', 'Columbus', 'WashingtonDC']\n",
    "    sent_dfs = []\n",
    "    for site in sites:\n",
    "        sent_df = joblib.load(join(\n",
    "            data.PROCESSED_DIR, f'sentiments_df_{site}_{checkpoint.split(\"/\")[-1]}.pkl'))\n",
    "        sent_dfs.append(sent_df)\n",
    "\n",
    "    sent_dfs[0].columns = np.arange(0, 11)\n",
    "    sent_dfs[1].columns = np.arange(11, 22)\n",
    "    sent_dfs[2].columns = np.arange(22, 33)\n",
    "    sent_llm_full = pd.concat(sent_dfs, axis=1).values\n",
    "    annotations_df[checkpoint] = annotations_df.apply(\n",
    "        lambda row: sent_llm_full[row['Question number'], row['Response number']], axis=1)\n",
    "    annotations_df[checkpoint] = annotations_df[checkpoint].apply(\n",
    "        find_starting_number)\n",
    "llms_to_ensemble = ['gpt-4', 'gpt-35-turbo',\n",
    "                    'mistralai/Mistral-7B-v0.1',  # 'meta-llama/Llama-2-70b-hf',\n",
    "                    # 'mistralai/Mixtral-8x7B-v0.1',\n",
    "                    ]\n",
    "\n",
    "annotations_df['Human ensemble'] = annotations_df[[\n",
    "    'hum1', 'hum2', 'hum3']].mean(axis=1)\n",
    "annotations_df = annotations_df.rename(columns={\n",
    "    'hum1': 'Human 1',\n",
    "    'hum2': 'Human 2',\n",
    "    'hum3': 'Human 3',\n",
    "})\n",
    "\n",
    "\n",
    "annotations_df.columns = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), annotations_df.columns))\n",
    "checkpoints_all = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), checkpoints_all))\n",
    "llms_to_ensemble = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "annotations_df['LLM ensemble'] = annotations_df[llms_to_ensemble].mean(axis=1)\n",
    "\n",
    "\n",
    "# note: GPT-4 and GPT-3.5 Turbo got swapped while running, need to swap their column vals back\n",
    "col_gpt4 = annotations_df['GPT-4'].copy()\n",
    "col_gpt35 = annotations_df['GPT-3.5 Turbo'].copy()\n",
    "annotations_df['GPT-4'] = col_gpt35\n",
    "annotations_df['GPT-3.5 Turbo'] = col_gpt4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute correlations between the columns of `annotations_df` and store in `corr_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to apply this across all llms\n",
    "notna = np.ones(len(annotations_df), dtype=bool)\n",
    "for ckpt in checkpoints_all:\n",
    "    notna &= annotations_df[ckpt].apply(find_starting_number).notna()\n",
    "print('num annots', notna.sum())\n",
    "\n",
    "checkpoints_all_hum = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble'] + \\\n",
    "    ['LLM ensemble'] + checkpoints_all\n",
    "corr = np.zeros((len(checkpoints_all_hum), len(checkpoints_all_hum)))\n",
    "for r, cr in enumerate(checkpoints_all_hum):\n",
    "    for c, cc in enumerate(checkpoints_all_hum):\n",
    "        x = annotations_df[cr][notna]\n",
    "        y = annotations_df[cc][notna]\n",
    "        corr[r, c] = np.corrcoef(x, y)[0, 1]  # spearmanr(x, y)\n",
    "        # corr[r, c] = cohen_kappa_score(x, y)\n",
    "\n",
    "# convert to df\n",
    "\n",
    "# labels[0] = 'Human 1'\n",
    "# labels[1] = 'Human 2'\n",
    "# labels[2] = 'Human 3'\n",
    "labels = checkpoints_all_hum\n",
    "print(labels, corr.shape)\n",
    "corr_df = pd.DataFrame(corr,\n",
    "                       index=labels,\n",
    "                       columns=labels)\n",
    "\n",
    "# sort by corr with human\n",
    "ind = corr_df.sort_values(by='Human ensemble', ascending=False).index\n",
    "ind.values[:4] = ['Human 1', 'Human 2', 'Human 3', 'Human ensemble']\n",
    "corr_df = corr_df.reindex(ind)[ind]\n",
    "\n",
    "# Replace correlations with Human ensemble by excluding the human\n",
    "cols = ['Human 1', 'Human 2', 'Human 3']\n",
    "hum_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = annotations_df[[\n",
    "        col for col in cols if col != c]].mean(axis=1)\n",
    "    hum_corrs.append(np.corrcoef(\n",
    "        annotations_df[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(hum_corrs)\n",
    "corr_df.loc[cols, 'Human ensemble'] = hum_corrs\n",
    "corr_df.loc['Human ensemble', cols] = hum_corrs\n",
    "\n",
    "# Replace correlations with LLM ensemble by excluding the LLM\n",
    "cols = list(map(lambda x: imodelsx.viz.CHECKPOINTS_RENAME_DICT.get(\n",
    "    x, x), llms_to_ensemble))\n",
    "llm_corrs = []\n",
    "for i, c in enumerate(cols):\n",
    "    avg_excluding_c = annotations_df[[\n",
    "        col for col in cols if col != c]].mean(axis=1)\n",
    "    llm_corrs.append(np.corrcoef(\n",
    "        annotations_df[c][notna], avg_excluding_c[notna])[0, 1])\n",
    "# print(llm_corrs)\n",
    "corr_df.loc[cols, 'LLM ensemble'] = llm_corrs\n",
    "corr_df.loc['LLM ensemble', cols] = llm_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make main plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_df = corr_df.iloc[1:, 1:]\n",
    "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
    "mask[np.diag_indices_from(mask)] = False\n",
    "\n",
    "rename = {\n",
    "    'Human 1': 'Human reviewer 1',\n",
    "    'Human 2': 'Human reviewer 2',\n",
    "    'Human 3': 'Human reviewer 3',\n",
    "    'Human ensemble': 'Mean human reviewer',\n",
    "    'LLM ensemble': 'Mean LLM',\n",
    "}\n",
    "corr_df = corr_df.rename(columns=rename, index=rename)\n",
    "ax = sns.heatmap(\n",
    "    corr_df,\n",
    "    annot=True, fmt='.2f',\n",
    "    cmap=sns.color_palette(\"Blues\", as_cmap=True), cbar_kws={'label': 'Correlation'},\n",
    "    mask=mask,\n",
    ")\n",
    "\n",
    "# outline the first row of the elements in the heatmap\n",
    "color = '#fa755a'\n",
    "lw = 3\n",
    "roffset = 3.5\n",
    "coffset = 0.5\n",
    "shape = corr_df.shape\n",
    "r = 0\n",
    "# color = 'gray'\n",
    "alpha = 1\n",
    "for c in range(3, shape[1]):\n",
    "    rx = r + roffset\n",
    "    cx = c + coffset\n",
    "    if c == 2:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx - 0.5, cx - 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    if c == shape[1] - 1:\n",
    "        plt.plot([rx - 0.5, rx + 0.5],\n",
    "                 [cx + 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx - 0.5, rx - 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "    plt.plot([rx + 0.5, rx + 0.5],\n",
    "             [cx - 0.5, cx + 0.5], color=color, lw=lw, alpha=alpha)\n",
    "\n",
    "roffset = 0.5\n",
    "coffset = 0.5\n",
    "c = 3\n",
    "for r in range(0, 4):\n",
    "    kwargs = {\n",
    "        'color': color,\n",
    "        'lw': lw,\n",
    "        # 'linestyle': '-',\n",
    "        # 'marker': 'None',\n",
    "        # 'alpha': 0.5,\n",
    "    }\n",
    "    rx = r + roffset\n",
    "    cx = c + coffset\n",
    "    # if c == 2:\n",
    "    plt.plot([rx - 0.5, rx + 0.5],\n",
    "             [cx - 0.5, cx - 0.5], **kwargs)\n",
    "    # if c == shape[1] - 1:\n",
    "    plt.plot([rx - 0.5, rx + 0.5],\n",
    "             [cx + 0.5, cx + 0.5], **kwargs)\n",
    "    if r == 0:\n",
    "        plt.plot([rx - 0.5, rx - 0.5],\n",
    "                 [cx - 0.5, cx + 0.5], **kwargs)\n",
    "    if r == 3:\n",
    "        plt.plot([rx + 0.5, rx + 0.5],\n",
    "                 [cx - 0.5, cx + 0.5], **kwargs)\n",
    "\n",
    "# set the color of first three xticklabels and yticklabels to blue\n",
    "for i, t in enumerate(ax.get_xticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "for i, t in enumerate(ax.get_yticklabels()):\n",
    "    if i < 4:\n",
    "        t.set_color('#08346c')\n",
    "\n",
    "\n",
    "plt.xlim(-.2, shape[0])\n",
    "plt.ylim(shape[1] + 0.2, -.2)\n",
    "# plt.ylabel('Annotator')\n",
    "# plt.xlabel('Annotator')\n",
    "plt.savefig('../figs/sentiment_correlation.pdf', bbox_inches='tight')\n",
    "plt.savefig('../figs/sentiment_correlation.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_confidence_interval(r, alpha, n):\n",
    "    def _r_to_z(r):\n",
    "        return math.log((1 + r) / (1 - r)) / 2.0\n",
    "\n",
    "    def _z_to_r(z):\n",
    "        e = math.exp(2 * z)\n",
    "        return ((e - 1) / (e + 1))\n",
    "\n",
    "    z = _r_to_z(r)\n",
    "    se = 1.0 / math.sqrt(n - 3)\n",
    "    z_crit = scipy.stats.norm.ppf(1 - alpha/2)  # 2-tailed z critical value\n",
    "\n",
    "    lo = z - z_crit * se\n",
    "    hi = z + z_crit * se\n",
    "\n",
    "    # Return a sequence\n",
    "    return (round(_z_to_r(lo), 2), round(_z_to_r(hi), 2))\n",
    "\n",
    "\n",
    "print('interval human-LLM', r_confidence_interval(0.741484, 0.05, 123))\n",
    "print('interval annotators', np.mean([0.813302, 0.850014, 0.935228]), r_confidence_interval(\n",
    "    np.mean([0.813302, 0.850014, 0.935228]), 0.05, 123))\n",
    "print('interval each human-LLM',\n",
    "      [r_confidence_interval(x, 0.05, 123) for x in [0.845929, 0.915878, 0.941113]])\n",
    "print('interval human1-human2',\n",
    "      [r_confidence_interval(0.813302, 0.05, 123)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = corr_df.loc['Mean human reviewer', [\n",
    "    'Human reviewer 1', 'Human reviewer 2', 'Human reviewer 3']]\n",
    "print('Human ensemble vs human', vals.mean(), vals.sem(ddof=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = annotations_df['GPT-4'][notna]\n",
    "x2 = annotations_df['Human ensemble'][notna]\n",
    "\n",
    "# calculate pearson correlation and error of the corr\n",
    "corr = np.corrcoef(x1, x2)[0, 1]\n",
    "err = np.sqrt((1 - corr**2) / (len(x1) - 2))\n",
    "print('GPT-4 vs human ensemble corr', corr, 'err', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with some jitter\n",
    "def jitter(values, j, min=1, max=5):\n",
    "    values = values + np.random.uniform(-j, j, values.shape)\n",
    "    return np.clip(values, min, max)\n",
    "\n",
    "\n",
    "x = annotations_df['Human ensemble']\n",
    "y = annotations_df['GPT-4']\n",
    "plt.plot(jitter(x, 0.15), jitter(y, 0.15), 'o', alpha=0.5)\n",
    "plt.xlabel('Annotator ensemble sentiment score')\n",
    "plt.ylabel('GPT-4 ensemble sentiment score')\n",
    "\n",
    "plt.plot([1, 5], [1, 5], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fraction of time GPT-4 is more extreme than human',\n",
    "      (np.abs(y[notna] - 3) > np.abs(x[notna] - 3)).mean())\n",
    "print('Fraction of time GPT-4 reverses polarity',\n",
    "      (((x >= 4) & (y <= 2)) | ((x <= 2) & (y >= 4)))[notna].mean()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at errors\n",
    "annots = annotations_df[['Question', 'Response', 'Human ensemble', 'GPT-4']]\n",
    "annots['diff'] = annots['Human ensemble'] - annots['GPT-4']\n",
    "annots = annots.sort_values(by='diff', ascending=False)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
